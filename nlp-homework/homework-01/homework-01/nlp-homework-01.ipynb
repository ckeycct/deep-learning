{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Can you come up out 3 sceneraies which use AI methods?\n",
    "\n",
    "Ans: 语义分割、语音识别、语义推断\n",
    "1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "\n",
    "Ans: Github便于存储、共享代码，相当于一个对程序员特化的网盘；Jupyter便于编译、演示代码和添加文字、公式说明；Pycharm比起Jupyter更善于用于大型项目，对各种包的管理也方便。\n",
    "2. What's the Probability Model?\n",
    "\n",
    "Ans:每个单词都有由频率为无偏估计的出现概率；句子拆为单词的时间序列后如果看作马         氏链可以有一个条件概率模型。\n",
    "3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "\n",
    "Ans:对一个特化场景的问答，如饭店应对顾客预约的电话，收集数据后可以很容易算出不         同的回答对某个提问的出现概率，选概率高的作为自动回答。\n",
    "4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "\n",
    "Ans:语义分析是本质的困难，机器没法像人一样综合利用经验、推断进行复杂的思考\n",
    "5. What's the Language Model;\n",
    "\n",
    "Ans:将句子看作词组按在句子中出现顺序的马氏链的概率模型。有了语料库后可以算出每         个句子的出现概率。\n",
    "6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "\n",
    "Ans:同3.\n",
    "7. What's the 1-gram language model;\n",
    "\n",
    "Ans:每个词组是独立的，句子的概率等于所有词组的概率乘积。\n",
    "8. What's the disadvantages and advantages of 1-gram language model;\n",
    "\n",
    "Ans:优点是简单好算，缺点是没有实际效果，体现不出常见词组搭配，生成效果会很差。\n",
    "9. What't the 2-gram models;\n",
    "\n",
    "Ans:每个词组出现概率依赖前一个词组。假设sentence为w1,w2,...wN按顺序组成的，有\n",
    "$$P(w_2|w_1)=\\dfrac{P(w_2w_1)}{P(w_1)}$$\n",
    "$$P(sentence)=\\prod_{i=1}^{i=N-1}P(w_{i+1}|w_{i})P(w_1)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编程实践部分"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. 设计你自己的句子生成器\n",
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。\n",
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "一个“接待员”的语言可以定义为\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "请定义你自己的语法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from jieba import cut\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'host': [['寒暄', '报数', '询问', '业务相关', '结尾']],\n",
       " '报数': [['我是', '数字', '号', ',']],\n",
       " '数字': [['单个数字'], ['数字', '单个数字']],\n",
       " '单个数字': [['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']],\n",
       " '寒暄': [['称谓', '打招呼'], ['打招呼']],\n",
       " '称谓': [['人称', ',']],\n",
       " '人称': [['先生'], ['女士'], ['小朋友']],\n",
       " '打招呼': [['你好'], ['您好']],\n",
       " '询问': [['请问你要'], ['您需要']],\n",
       " '业务相关': [['玩玩', '具体业务']],\n",
       " '玩玩': [['耍一耍'], ['玩一玩']],\n",
       " '具体业务': [['喝酒'], ['打牌'], ['打猎'], ['赌博']],\n",
       " '结尾': [['吗？']]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'human': [['自己', '寻找', '活动']],\n",
       " '自己': [['我'], ['俺'], ['我们']],\n",
       " '寻找': [['看看'], ['找找'], ['想找点']],\n",
       " '活动': [['乐子'], ['玩的']]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_grammar(gram,line_split='\\n',split='='):\n",
    "    #生成语法，这是一个python字典，之后可以用其树结构快速生成例句。树一般有三层，\n",
    "    #由上往下依次是语法名、句子结构、用于填充每个结构的备选词集合。\n",
    "    dicts={}\n",
    "    for line in gram.split(line_split):\n",
    "        if not line.strip():continue\n",
    "        else:\n",
    "            index,statement=line.split(split)\n",
    "            dicts[index.strip()]=[s.split() for s in statement.split('|')]\n",
    "            #这里要确保动作index(如host、报数、数字)的statement的list长度为1\n",
    "            #但其他index的statement的list，每个状态贡献一个length\n",
    "    return dicts\n",
    "host_gram=create_grammar(host)\n",
    "host_gram\n",
    "human_gram=create_grammar(human)\n",
    "human_gram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。\n",
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小朋友,您好我是65号,您需要耍一耍打牌吗？'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator(gram,target):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    else:\n",
    "        expand = random.choice(gram[target])\n",
    "        expand_recursion=[ generator(gram,t) for t in expand ]\n",
    "        return ''.join(e for e in expand_recursion)\n",
    "generator(host_gram,'host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我们想找点玩的', '我们找找玩的', '我想找点玩的']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gernerator_n(gram,target,n=1):\n",
    "    return [generator(gram,target) for i in range(n)]\n",
    "gernerator_n(human_gram,'human',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用新数据源完成语言模型的训练 \\\n",
    "按照我们上文中定义的prob_2函数，我们更换一个文本数据源，获得新的Language Model:\\\n",
    "下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\\\n",
    "可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz \\\n",
    "可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv \\\n",
    "修改代码，获得新的2-gram语言模型 \\\n",
    "进行文本清洗，获得所有的纯文本 \\\n",
    "将这些文本进行切词 \\\n",
    "送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吴京意淫到了脑残的地步看了恶心想吐首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视物理逻辑于不顾不得不说有钱真好随意胡闹吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了煽情而煽情让人觉得他是个大做作大谎言家729更新片子整体不如湄公河行动1整体不够流畅编剧有毒台词尴尬2刻意做作的主旋律煽情显得如此不合时宜而又多'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#尝试了多种清洗数据的方式，步骤包含取出纯中文、去掉nan(用pandas可能涉及)\n",
    "#第一种不好去掉数字和句子中英文\n",
    "forshow_data_source=pd.read_csv('movie_comments.csv',low_memory=False)\n",
    "forshow_data_source=forshow_data_source['comment'].tolist()\n",
    "forshow_data=''\n",
    "for i in forshow_data_source:\n",
    "    if str(i) != 'nan':\n",
    "        forshow_data=forshow_data+i\n",
    "forshow_data=''.join(re.findall('\\w+',forshow_data))\n",
    "forshow_data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'战狼吴京意淫到了脑残的地步看了恶心想吐战狼首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视物理逻辑于不顾不得不说有钱真好随意胡闹战狼吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了煽情而煽情让人觉得他是个大做作大谎言家更新片子整体不如湄公河行动整体不够流畅编剧有毒台词尴尬刻意做作的主旋律煽情显得如此不合时宜而又'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第二种糙快猛，就用它了:\n",
    "with open('movie_comments.csv') as f:\n",
    "    L=f.readlines()\n",
    "    pattern=\"[\\u4e00-\\u9fa5]+\" \n",
    "    regex = re.compile(pattern)\n",
    "    data1=[]\n",
    "    for i in L:\n",
    "        results =  regex.findall(i)\n",
    "        data1+=results\n",
    "data1=''.join(data1)\n",
    "with open('train.txt') as f:\n",
    "    L=f.readlines()\n",
    "    pattern=\"[\\u4e00-\\u9fa5]+\" \n",
    "    regex = re.compile(pattern)\n",
    "    data2=[]\n",
    "    for i in L:\n",
    "        results =  regex.findall(i)\n",
    "        data2+=results\n",
    "data2=''.join(data2)\n",
    "data=data1+data2\n",
    "data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/7g/b3sd1r7d37j4ttzm2q7xpcwr0000gn/T/jieba.cache\n",
      "Loading model cost 0.993 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "data=list(cut(data))\n",
    "token_2_gram=[''.join(data[i:i+2]) for i in range(len(data[:-2]))]\n",
    "word_count_2=Counter(token_2_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    return words_count[word]/len(token_2_gram)\n",
    "def prob_2(word1,word2):\n",
    "    if word1+word2 in word_count_2:\n",
    "        return word_count_2[word1+word2]/len(token_2_gram)\n",
    "    else:\n",
    "        return 1/len(token_2_gram)\n",
    "def sentence_prob(sentence):\n",
    "    words = list(cut(sentence))\n",
    "    sentence_prob = 1\n",
    "    for i in range(len(words)-1):\n",
    "        word=words[i]\n",
    "        next_word=words[i+1]\n",
    "        probability=prob_2(word,next_word)\n",
    "        sentence_prob*=probability\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获得最优质的的语言\n",
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成n个句子，并能选择一个最合理的句子:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['您好我是18号,您需要耍一耍喝酒吗？', 8.01200900411323e-77],\n",
       " ['你好我是69号,请问你要玩一玩打牌吗？', 9.150328392747928e-80],\n",
       " ['先生,你好我是2号,您需要耍一耍赌博吗？', 1.7869253168815048e-89],\n",
       " ['小朋友,您好我是21号,您需要耍一耍赌博吗？', 2.978208861469175e-90],\n",
       " ['先生,您好我是56号,您需要耍一耍喝酒吗？', 2.978208861469175e-90],\n",
       " ['先生,你好我是532593号,请问你要玩一玩喝酒吗？', 3.401342795626461e-93],\n",
       " ['小朋友,您好我是88号,请问你要玩一玩打猎吗？', 5.668904659377435e-94],\n",
       " ['先生,您好我是6号,请问你要玩一玩喝酒吗？', 5.668904659377435e-94],\n",
       " ['小朋友,你好我是3号,请问你要耍一耍打牌吗？', 1.1337809318754873e-94],\n",
       " ['先生,您好我是7号,请问你要耍一耍喝酒吗？', 1.889634886459145e-95]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_best(gram=host_gram,target='host',n=10):\n",
    "    result=[]\n",
    "    for i in range(n):\n",
    "        sentence=generator(gram,target)\n",
    "        result.append([sentence,sentence_prob(sentence)])\n",
    "    return sorted(result,key=lambda x :x[1],reverse=True)\n",
    "generate_best()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？\n",
    "Ans:扩充语料库，使其按人类使用频率为权重地均匀覆盖各个话题、领域。其实社交软件聊天记录是很好的语料库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
